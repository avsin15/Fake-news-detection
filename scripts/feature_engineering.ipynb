{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89909cf0-5c1e-495d-8016-5ea3c9bea1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-12 13:55:51,593 - INFO - üöÄ Starting Feature Engineering Pipeline\n",
      "2025-10-12 13:55:51,593 - INFO - ============================================================\n",
      "2025-10-12 13:55:51,596 - INFO - üî¢ Random seed set to 42\n",
      "2025-10-12 13:55:51,596 - INFO - üì• Loading data from data/processed/fake_news_clean.csv\n",
      "2025-10-12 13:55:54,538 - INFO - ‚úÖ Loaded 72134 rows\n",
      "2025-10-12 13:55:54,539 - INFO - üîç Validating data...\n",
      "2025-10-12 13:55:54,546 - WARNING - ‚ö†Ô∏è Found null values:\n",
      "clean_text    917\n",
      "label           0\n",
      "dtype: int64\n",
      "2025-10-12 13:55:54,551 - INFO - ‚úÖ Removed null values. Remaining rows: 71217\n",
      "2025-10-12 13:55:54,553 - INFO - üè∑Ô∏è Encoding labels...\n",
      "2025-10-12 13:55:54,554 - INFO - Original label distribution:\n",
      "label\n",
      "1    36191\n",
      "0    35026\n",
      "Name: count, dtype: int64\n",
      "2025-10-12 13:55:54,571 - INFO - Encoded label distribution:\n",
      "label\n",
      "1    36191\n",
      "0    35026\n",
      "Name: count, dtype: int64\n",
      "2025-10-12 13:55:54,572 - INFO - üß† Using device: cpu\n",
      "2025-10-12 13:55:54,572 - INFO - ‚öôÔ∏è Loading model: sentence-transformers/all-MiniLM-L6-v2\n",
      "2025-10-12 13:55:54,575 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "2025-10-12 13:55:58,472 - INFO - ‚öôÔ∏è Generating embeddings (this may take a few minutes)...\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1113/1113 [13:10<00:00,  1.41it/s]\n",
      "2025-10-12 14:09:12,119 - INFO - ‚úÖ Embeddings created: shape (71217, 384), dtype float32\n",
      "2025-10-12 14:09:12,121 - INFO - üìä Splitting data into train/validation/test sets...\n",
      "2025-10-12 14:09:12,191 - INFO - ‚úÖ Train: 49851 | Val: 7121 | Test: 14245\n",
      "2025-10-12 14:09:12,196 - INFO - Train label dist ‚Üí Real=24518, Fake=25333\n",
      "2025-10-12 14:09:12,197 - INFO - Val label dist ‚Üí Real=3502, Fake=3619\n",
      "2025-10-12 14:09:12,199 - INFO - Test label dist ‚Üí Real=7006, Fake=7239\n",
      "2025-10-12 14:09:12,200 - INFO - üíæ Saving features and metadata...\n",
      "2025-10-12 14:09:12,200 - INFO - üìè Scaling embeddings using StandardScaler...\n",
      "2025-10-12 14:09:12,340 - INFO - ‚úÖ Scaler saved ‚Üí data/processed/features/scaler.pkl\n",
      "2025-10-12 14:09:15,640 - INFO - ‚úÖ Features saved ‚Üí data/processed/features/features.npz\n",
      "2025-10-12 14:09:15,695 - INFO - ‚úÖ Train/Val/Test sets saved as .pkl files\n",
      "2025-10-12 14:09:15,702 - INFO - ‚úÖ Metadata saved ‚Üí data/processed/features/metadata.json\n",
      "2025-10-12 14:09:15,704 - INFO - \n",
      "============================================================\n",
      "2025-10-12 14:09:15,704 - INFO - üìà FEATURE ENGINEERING SUMMARY\n",
      "2025-10-12 14:09:15,704 - INFO - ============================================================\n",
      "2025-10-12 14:09:15,705 - INFO - Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "2025-10-12 14:09:15,705 - INFO - Embedding Dimension: 384\n",
      "2025-10-12 14:09:15,706 - INFO - Train Samples: 49851\n",
      "2025-10-12 14:09:15,706 - INFO - Val Samples: 7121\n",
      "2025-10-12 14:09:15,707 - INFO - Test Samples: 14245\n",
      "2025-10-12 14:09:15,707 - INFO - Scaler Used: True\n",
      "2025-10-12 14:09:15,707 - INFO - Generated On: 2025-10-12T14:09:15.695526\n",
      "2025-10-12 14:09:15,707 - INFO - ============================================================\n",
      "\n",
      "2025-10-12 14:09:15,707 - INFO - ‚úÖ Feature engineering completed successfully!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Feature Engineering for Fake News Detection\n",
    "---------------------------------------------------\n",
    "- Loads cleaned data\n",
    "- Converts text into dense embeddings using SentenceTransformer\n",
    "- Creates train/test split with validation set\n",
    "- Scales features and saves metadata\n",
    "- Includes GPU support, reproducibility, and detailed logging\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "PROCESSED_PATH = \"data/processed/fake_news_clean.csv\"\n",
    "FEATURE_DIR = \"data/processed/features\"\n",
    "METADATA_FILE = os.path.join(FEATURE_DIR, \"metadata.json\")\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "BATCH_SIZE = 64\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.1\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ============================================================\n",
    "# LOGGING SETUP\n",
    "# ============================================================\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ============================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================\n",
    "def validate_data(df):\n",
    "    \"\"\"Validate that required columns exist and data is not empty.\"\"\"\n",
    "    logger.info(\"üîç Validating data...\")\n",
    "    if df.empty:\n",
    "        raise ValueError(\"‚ùå Dataset is empty!\")\n",
    "\n",
    "    required_cols = [\"clean_text\", \"label\"]\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"‚ùå Missing columns: {missing_cols}\")\n",
    "\n",
    "    # Handle nulls\n",
    "    null_counts = df[required_cols].isnull().sum()\n",
    "    if null_counts.any():\n",
    "        logger.warning(f\"‚ö†Ô∏è Found null values:\\n{null_counts}\")\n",
    "        df = df.dropna(subset=required_cols)\n",
    "        logger.info(f\"‚úÖ Removed null values. Remaining rows: {len(df)}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def encode_labels(df):\n",
    "    \"\"\"Encode labels to binary (0 = real, 1 = fake).\"\"\"\n",
    "    logger.info(\"üè∑Ô∏è Encoding labels...\")\n",
    "    logger.info(f\"Original label distribution:\\n{df['label'].value_counts()}\")\n",
    "\n",
    "    df[\"label\"] = df[\"label\"].apply(\n",
    "        lambda x: 1 if str(x).lower() in [\"fake\", \"1\", \"true\"] else 0\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Encoded label distribution:\\n{df['label'].value_counts()}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_embeddings(texts, model_name=MODEL_NAME, batch_size=BATCH_SIZE):\n",
    "    \"\"\"Generate embeddings using SentenceTransformer.\"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    logger.info(f\"üß† Using device: {device}\")\n",
    "    logger.info(f\"‚öôÔ∏è Loading model: {model_name}\")\n",
    "\n",
    "    try:\n",
    "        model = SentenceTransformer(model_name, device=device)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Failed to load model: {e}\")\n",
    "        raise\n",
    "\n",
    "    logger.info(\"‚öôÔ∏è Generating embeddings (this may take a few minutes)...\")\n",
    "    try:\n",
    "        embeddings = model.encode(\n",
    "            texts,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Failed to generate embeddings: {e}\")\n",
    "        raise\n",
    "\n",
    "    logger.info(f\"‚úÖ Embeddings created: shape {embeddings.shape}, dtype {embeddings.dtype}\")\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def split_data(X, y, test_size=TEST_SIZE, val_size=VAL_SIZE, random_state=RANDOM_STATE):\n",
    "    \"\"\"Split data into train, validation, and test sets.\"\"\"\n",
    "    logger.info(\"üìä Splitting data into train/validation/test sets...\")\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y,\n",
    "        test_size=(test_size + val_size),\n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    val_ratio = val_size / (test_size + val_size)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp,\n",
    "        test_size=(1 - val_ratio),\n",
    "        random_state=random_state,\n",
    "        stratify=y_temp\n",
    "    )\n",
    "\n",
    "    logger.info(f\"‚úÖ Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n",
    "    logger.info(f\"Train label dist ‚Üí Real={sum(y_train==0)}, Fake={sum(y_train==1)}\")\n",
    "    logger.info(f\"Val label dist ‚Üí Real={sum(y_val==0)}, Fake={sum(y_val==1)}\")\n",
    "    logger.info(f\"Test label dist ‚Üí Real={sum(y_test==0)}, Fake={sum(y_test==1)}\")\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "def save_features(X_train, X_val, X_test, y_train, y_val, y_test, output_dir=FEATURE_DIR):\n",
    "    \"\"\"Save features, scaler, and metadata.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    logger.info(\"üíæ Saving features and metadata...\")\n",
    "\n",
    "    # ============================================================\n",
    "    # Scale embeddings for ML models\n",
    "    # ============================================================\n",
    "    logger.info(\"üìè Scaling embeddings using StandardScaler...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    scaler_file = os.path.join(output_dir, \"scaler.pkl\")\n",
    "    joblib.dump(scaler, scaler_file)\n",
    "    logger.info(f\"‚úÖ Scaler saved ‚Üí {scaler_file}\")\n",
    "\n",
    "    # Save data arrays\n",
    "    npz_file = os.path.join(output_dir, \"features.npz\")\n",
    "    np.savez_compressed(\n",
    "        npz_file,\n",
    "        X_train=X_train, X_val=X_val, X_test=X_test,\n",
    "        y_train=y_train, y_val=y_val, y_test=y_test\n",
    "    )\n",
    "    logger.info(f\"‚úÖ Features saved ‚Üí {npz_file}\")\n",
    "\n",
    "    # Save pickled datasets\n",
    "    joblib.dump((X_train, y_train), os.path.join(output_dir, \"train.pkl\"))\n",
    "    joblib.dump((X_val, y_val), os.path.join(output_dir, \"val.pkl\"))\n",
    "    joblib.dump((X_test, y_test), os.path.join(output_dir, \"test.pkl\"))\n",
    "    logger.info(\"‚úÖ Train/Val/Test sets saved as .pkl files\")\n",
    "\n",
    "    # Metadata\n",
    "    metadata = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"embedding_dim\": X_train.shape[1],\n",
    "        \"train_samples\": X_train.shape[0],\n",
    "        \"val_samples\": X_val.shape[0],\n",
    "        \"test_samples\": X_test.shape[0],\n",
    "        \"total_samples\": len(X_train) + len(X_val) + len(X_test),\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "        \"scaler_used\": True,\n",
    "        \"val_split_ratio\": VAL_SIZE,\n",
    "        \"test_split_ratio\": TEST_SIZE,\n",
    "        \"train_real\": int(sum(y_train == 0)),\n",
    "        \"train_fake\": int(sum(y_train == 1)),\n",
    "        \"val_real\": int(sum(y_val == 0)),\n",
    "        \"val_fake\": int(sum(y_val == 1)),\n",
    "        \"test_real\": int(sum(y_test == 0)),\n",
    "        \"test_fake\": int(sum(y_test == 1))\n",
    "    }\n",
    "\n",
    "    with open(METADATA_FILE, \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "\n",
    "    logger.info(f\"‚úÖ Metadata saved ‚Üí {METADATA_FILE}\")\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def print_summary(metadata):\n",
    "    \"\"\"Print summary of the process.\"\"\"\n",
    "    logger.info(\"\\n\" + \"=\"*60)\n",
    "    logger.info(\"üìà FEATURE ENGINEERING SUMMARY\")\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(f\"Model: {metadata['model_name']}\")\n",
    "    logger.info(f\"Embedding Dimension: {metadata['embedding_dim']}\")\n",
    "    logger.info(f\"Train Samples: {metadata['train_samples']}\")\n",
    "    logger.info(f\"Val Samples: {metadata['val_samples']}\")\n",
    "    logger.info(f\"Test Samples: {metadata['test_samples']}\")\n",
    "    logger.info(f\"Scaler Used: {metadata['scaler_used']}\")\n",
    "    logger.info(f\"Generated On: {metadata['timestamp']}\")\n",
    "    logger.info(\"=\"*60 + \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN PIPELINE\n",
    "# ============================================================\n",
    "def main():\n",
    "    \"\"\"Main feature engineering pipeline.\"\"\"\n",
    "    logger.info(\"üöÄ Starting Feature Engineering Pipeline\")\n",
    "    logger.info(\"=\"*60)\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    torch.manual_seed(RANDOM_STATE)\n",
    "    logger.info(f\"üî¢ Random seed set to {RANDOM_STATE}\")\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(PROCESSED_PATH):\n",
    "            raise FileNotFoundError(f\"‚ùå Cleaned dataset not found at {PROCESSED_PATH}\")\n",
    "\n",
    "        logger.info(f\"üì• Loading data from {PROCESSED_PATH}\")\n",
    "        df = pd.read_csv(PROCESSED_PATH)\n",
    "        logger.info(f\"‚úÖ Loaded {len(df)} rows\")\n",
    "\n",
    "        # Validation & encoding\n",
    "        df = validate_data(df)\n",
    "        df = encode_labels(df)\n",
    "\n",
    "        # Embeddings\n",
    "        embeddings = generate_embeddings(df[\"clean_text\"].tolist())\n",
    "\n",
    "        # Split\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = split_data(\n",
    "            embeddings, df[\"label\"].values\n",
    "        )\n",
    "\n",
    "        # Save features\n",
    "        metadata = save_features(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "\n",
    "        # Print summary\n",
    "        print_summary(metadata)\n",
    "\n",
    "        logger.info(\"‚úÖ Feature engineering completed successfully!\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error during feature engineering: {e}\", exc_info=True)\n",
    "        return False\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    success = main()\n",
    "    exit(0 if success else 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c372c3-5052-4e12-a143-8e91f396fc17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
